{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec6f130",
   "metadata": {},
   "source": [
    "# Serial Optimization\n",
    "How do you optimise serial code? We cover some of the techniques that you might use for serial optimization in Python: \n",
    "1. Vectorization\n",
    "2. Memoization\n",
    "3. Compilation\n",
    "\n",
    "## Vectorization\n",
    "Because Python is an interpreted language - i.e. each line is dispatched to an interpreter program to interpret and execute - there can be a lot of overhead compared to compiled programs. Python needs to check variable types and use the correct functions for the inputs, because these are not necessarily specified. As a result, when you're using Python for large datasets or for long loops, it can pay to implement vectorization.\n",
    "\n",
    "Vectorization essentially increases memory locality, and allows the CPU to use SIMD instructions.  Single Instruction, Multiple Data (SIMD) instructions - these allow the same instruction to be performed on multiple cells of data simultaneously. There is therefore much less overhead per operation, providing large speedups. Improving memory locality also reduces the overhead of dereferencing lots of values that are in lots of different places in memory - contiguous memory blocks that contain all the data you need are loaded on to the CPU instead.\n",
    "\n",
    "Consider the following examples. The first uses base Python to generate a list of integers using a for loop. The second does the same, but uses `numpy` to vectorize the calculation. The `%%timeit` readouts show a large speed increase in the second example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20a127ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ints():\n",
    "  result = []\n",
    "  for x in range(1_000_000):\n",
    "    result.append(x * 2)\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d8c0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.2 ms ± 711 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit double_ints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e887ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "263669b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_double_ints():\n",
    "  ints = np.arange(1_000_000)\n",
    "  return ints * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8b59c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert double_ints() == np_double_ints().tolist(), \"Results do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9c12b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 ms ± 28.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np_double_ints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f804c",
   "metadata": {},
   "source": [
    "We see a large speed increase in the second example, which uses `numpy` to vectorize the calculation.\n",
    "\n",
    "### Vectorization Exercises\n",
    "The follow code calculates $\\pi$ using Monte Carlo integration. Answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef054aa",
   "metadata": {},
   "source": [
    "The following code, which calculates pi using Monte Carlo integration, is provided. Answer the following questions:\n",
    "1. Is it correct?\n",
    "2. How long does it take to run?\n",
    "3. Where is the most time taken, cumulatively?\n",
    "\n",
    "Then:\n",
    "1. Vectorize the code\n",
    "2. Verify the outputs\n",
    "3. Measure any speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff424d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def monte_carlo_pi(num_samples):\n",
    "    inside_circle = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if x**2 + y**2 <= 1:\n",
    "            inside_circle += 1\n",
    "    return (inside_circle / num_samples) * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372fca2",
   "metadata": {},
   "source": [
    "**Q1.1: Is `monte_carlo_pi()` correct? At what number of samples are you accurate to three digits?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "Run the function with varying `num_samples` and decide whether, as `num_samples` increases, the output approaches 3.1415\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b6e3f",
   "metadata": {},
   "source": [
    "**Q1.2: How long does the function take to run, on average, when `num_samples = 1_000_000`? How does this scale as you vary the number of samples**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Use the `time` module or use `%timeit`\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "Write a wrapper function which runs and times the function.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "def pi_time(n):\n",
    "    import time\n",
    "    start = time.time()\n",
    "    out = monte_carlo_pi(n)\n",
    "    end = time.time()\n",
    "    return (out, end - start)\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c75e57",
   "metadata": {},
   "source": [
    "\n",
    "**Q1.3: What functions are called most in `monte_carlo_pi()`?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Use `cProfile`\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "Either use `%prun` magic, or call `cProfile()` directly.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "%prun monte_carlo_pi(10_000_000)\n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "```{Python}\n",
    "import cProfile\n",
    "cProfile.run(\"monte_carlo_pi(1_000_000)\")\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac57355",
   "metadata": {},
   "source": [
    "**Q1.4: Where is the most time taken in `monte_carlo_pi()`, cumulatively?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "You want to know which line takes most time.\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "Use `line_profiler` or `%lprun`.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "%load_ext line_profiler\n",
    "%lprun -f monte_carlo_pi monte_carlo_pi(10_000_000)\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827b4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.17552 s\n",
      "File: /tmp/ipykernel_2855/3916854312.py\n",
      "Function: monte_carlo_pi at line 3\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     3                                           def monte_carlo_pi(num_samples):\n",
      "     4         1        600.0    600.0      0.0      inside_circle = 0\n",
      "     5   1000001  108869600.0    108.9      9.3      for _ in range(num_samples):\n",
      "     6   1000000  700355101.0    700.4     59.6          x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
      "     7   1000000  261651600.0    261.7     22.3          if x**2 + y**2 <= 1:\n",
      "     8    784967  104646900.0    133.3      8.9              inside_circle += 1\n",
      "     9         1        800.0    800.0      0.0      return (inside_circle / num_samples) * 4"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f monte_carlo_pi monte_carlo_pi(1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b8532",
   "metadata": {},
   "source": [
    "\n",
    "**Q1.5: Can you speed up the function?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Previous results should have shown most of the time is taken generating random numbers, and checking whether the random point is inside a circle. Can you vectorise either of these?\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "`numpy` has functions that vectorise random number generation, and allows you to vectorise exponentiation.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "def monte_carlo_pi_numpy(num_samples):\n",
    "  x, y = np.random.uniform(-1, 1, (2, num_samples)) # vectorised random number generation\n",
    "  pi = 4 * np.mean(x**2 + y**2 <= 1) # perform check simultaneously on all datav`\\` has functions that vectorise random number generation, as wend allows you to vectorise exponentiation.pnumpy\n",
    "  return pi\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1530d9",
   "metadata": {},
   "source": [
    "\n",
    "**Q1.6: Is your edited function correct? Is it faster? Are there any other ways you can think of to speed it up?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193e991",
   "metadata": {},
   "source": [
    "## Memoization\n",
    "Memoization is a technique to cache results when they're calculated so they can be quickly retrieved when the same input is called. It avoids expensive computation of the same results again and again.\n",
    "\n",
    "Let's remind ourselves of the function we defined previously to calculate the $n^{th}$ number of the Fibonacci sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a0c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    if n <= 2:\n",
    "        return 1\n",
    "    return fib(n - 1) + fib(n - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed1aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.9 ms ± 450 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fib(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058fdb1",
   "metadata": {},
   "source": [
    "On our computer, `fib(35)` takes 892 ms on average.\n",
    "\n",
    "We can implement a cache simply ourselves, using dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fa41eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def fib_cache(n):\n",
    "    if n in cache:\n",
    "        return cache[n]\n",
    "    if n < 2:\n",
    "        result = n\n",
    "    else:\n",
    "        result = fib(n-1) + fib(n-2)\n",
    "    cache[n] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.7 ns ± 0.626 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "assert fib(35) == fib_cache(35), \"Cached result does not match non-cached result!\"\n",
    "%timeit fib_cache(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0512b31",
   "metadata": {},
   "source": [
    "On our computer, calling `fib_cache(35)` took an average of 61 nanoseconds. Merely implementing a basic cache resulted in approximately a 1-million-fold speedup.\n",
    "\n",
    "Due to the recursive nature of this function, the speedup achieved will be greater a n increases, due to the greater number of avoided computations as n grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda679b",
   "metadata": {},
   "source": [
    "We can also use the `functools` module, which comes with Python and provides the function decorator `@lru_cache`. LRU stands for Least Recently Used - when the cache hits its size limit it removes the least recently used data to make room for the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25eac273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fib_lru(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3612a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 ns ± 469 ns per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fib_lru(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d9d4a",
   "metadata": {},
   "source": [
    "On our computer, the cached version took 257 nanoseconds, *slower* than using our dictionary. Does that hold on your computer? Did you expect it? The functools version avoids code complexity and makes it very simple to cache, however, in this case it does not appear to be as fast as using a dictionary - reiterating the importance of measuring all changes to your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87261ac",
   "metadata": {},
   "source": [
    "### Memoization Exercises\n",
    "\n",
    "We provide a function below that computes the Binomial Coefficient (\"n choose k\") recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c349035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial(n, k):\n",
    "    if k == 0 or k == n:\n",
    "        return 1\n",
    "    return binomial(n - 1, k - 1) + binomial(n - 1, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eed2bd",
   "metadata": {},
   "source": [
    "\n",
    "**Q2.1: How long does this function take with n = 20, k = 10? Explore how the time taken changes as a function of n or k?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "Time the code with a variety of n/k.\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "Use %timeit, or write a wrapper function.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "Edit the code below and explore.\n",
    "```\n",
    "%timeit binomial(20, 10)\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3128a",
   "metadata": {},
   "source": [
    "\n",
    "**Q2.2: How many recursive function calls are there with n = 20, k = 10?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Use `cProfile`.\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "Either use `cProfile.run()` or `%prun`.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "%prun binomial(20, 10)\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a095dc",
   "metadata": {},
   "source": [
    "\n",
    "**Q2.3 Implement memoization. How fast is the function with the same inputs you tested before? How many function calls are there?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Cache the results if not already in the cache.\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "Either use a dictionary cache, or `@lru_cache` from `functools`, then time it and use `cProfile`.\n",
    "<details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```{Python}\n",
    "@lru_cache(maxsize=None)\n",
    "def binomial(n, k):\n",
    "    if k == 0 or k == n:\n",
    "        return 1\n",
    "    return binomial(n - 1, k - 1) + binomial(n - 1, k)\n",
    "\n",
    "%timeit binomial(20, 10)\n",
    "\n",
    "%prun binomial(20, 10)\n",
    "```\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba969b3",
   "metadata": {},
   "source": [
    "## Compilation of functions\n",
    "Python, as an interpreted language, has many overheads as we've discussed. To gain code flexibility and speed of implementation, the language had to make many dynamic checks as code is interpreted. The `CPython` interpreter is well implemented, and these checks are individually fairly fast, but over millions of computations the overhead adds up. In addition, there are implementation details that can simply make speedy code impossible - such as Python lists being arrays of pointers meaning every loop over them involves fetching data twice (get the pointer, then get the data where the pointer points to).\n",
    "\n",
    "There are, however, ways of stepping around the interpreter, and compiling our code ourselves for use in Python. The longest running and most mature method of this is `Cython`, which involves learning a small amount of `C` and setting up a toolchain. We will not be covering that, but if you're interested I encourage you to check it out at https://cython.org/.\n",
    "\n",
    "We will cover the `Numba` package. `Cython` is an Ahead-Of-Time (AOT) compiler - you compile your function(s) before using, creating a static library that can instantly be used. In comparison, `Numba` is a Just-In-Time (JIT) compiler - when your function is first called it will compile the function then, and *future* uses of that function will benefit from the compiled version. The first use, then, will be slow due to the compilation, with all subsequent calls being (hopefully) much faster.\n",
    "\n",
    "If you're using `Numba`, you simply add a decorator and away we go. There are two decorators to consider, because `Numba` has a `nopython` mode. In `nopython` mode the function is compiled to machine code without using the Python interpreter *at all* - however, if this is not possible it fails. Avoiding all the interpreter overhead is often worth it, though. \n",
    "\n",
    "- `@jit()` - tells `Numba` that you want this function to be compiled. It will try to use `nopython`, but if it fails it will fall back to using the python interpreter. Use `@jit(nopython=True)` to enforce `nopython` mode.\n",
    "- `@njit()` - tell `Numba` to compile the function in `nopython` mode.\n",
    "\n",
    "So, lets start using `Numba`. Below we code a Sieve of Eratosthenes using `numpy` to find all primes up to a number `n`. For a primer on the Sieve of Eratosthenes, read the [wikipedia page](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 μs ± 8.74 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def sieve_primes(n):\n",
    "    is_prime = np.ones(n + 1, dtype=np.uint8) # Create a boolean array\n",
    "    is_prime[:2] = 0  # 0 and 1 are not prime numbers\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if is_prime[i] == 1:\n",
    "            is_prime[i*i:n+1:i] = 0 # Mark multiples of i as non-prime\n",
    "    return np.where(is_prime)[0] # Get indices of True values, which are the prime numbers\n",
    "\n",
    "assert len(sieve_primes(10000)) == 1229, \"sieve_primes did not return expected number of primes!\"\n",
    "\n",
    "%timeit sieve_primes(100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64b36df",
   "metadata": {},
   "source": [
    "On our computer, the sieve of eratosthenes took 161 microseconds on average, when n = 100,000. \n",
    "\n",
    "Adding the `@jit` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c6f2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit()\n",
    "def sieve_primes_numba(n):\n",
    "    is_prime = np.ones(n + 1, dtype=np.uint8) # Create a boolean array\n",
    "    is_prime[:2] = 0  # 0 and 1 are not prime numbers\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if is_prime[i] == 1:\n",
    "            is_prime[i*i:n+1:i] = 0 # Mark multiples of i as non-prime\n",
    "    return np.where(is_prime)[0] # Get indices of True values, which are the prime numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10b49e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 μs ± 981 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "assert len(sieve_primes_numba(10000)) == 1229, \"sieve_primes did not return expected number of primes!\"\n",
    "\n",
    "%timeit sieve_primes_numba(100000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eba7b0",
   "metadata": {},
   "source": [
    "### Compilation exercises"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
