{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec6f130",
   "metadata": {},
   "source": [
    "# Serial Optimization\n",
    "How do you optimise serial code? There are two main techniques that are easy go-to's for serial optimization in Python: \n",
    "1. Vectorization\n",
    "2. Memoization\n",
    "\n",
    "## Vectorization\n",
    "Because Python is an interpreted language - i.e. each line is dispatched to an interpreter program to interpret and execute - there can be a lot of overhead compared to compiled programs. Python needs to check variable types and use the correct functions for the inputs, because these are not necessarily specified. As a result, when you're using Python for large datasets or for long loops, it can pay to implement vectorization.\n",
    "\n",
    "Vectorization essentially uses compiled versions of the loop or functions. There is therefore much less overhead per operation, providing large speedups. As an added benefit, they can use Single Instruction, Multiple Data (SIMD) instructions - these allow the same instruction to be performed on multiple cells of data simultaneously, providing an even greater speedup.\n",
    "\n",
    "Consider the following examples. The first uses base Python to generate a list of integers using a for loop. The second does the same, but uses `numpy` to vectorize the calculation. The %%timeit readouts show a large speed increase in the second example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a127ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.4 ms ± 232 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result = []\n",
    "for x in range(1_000_000):\n",
    "    result.append(x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e887ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263669b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.68 ms ± 67.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "ints = np.arange(1_000_000)\n",
    "result_np = ints * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738c52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result == result_np.tolist(), \"Results do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d1e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.4 ns ± 0.302 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit total = sum(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b90247a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 ns ± 0.737 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "import random\n",
    "\n",
    "def func():\n",
    "    total = 0\n",
    "    for _ in range(10):\n",
    "        total += random.randint(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193e991",
   "metadata": {},
   "source": [
    "## Memoization\n",
    "Memoization is a technique to cache results when they're calculated so they can be quickly retrieved when the same input is called. It avoids expensive computation of the same results again and again.\n",
    "\n",
    "We can implement a cache ourselves using basic Python, using a simple dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa41eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "def fib_cache(n):\n",
    "    if n in cache:\n",
    "        return cache[n]\n",
    "    if n < 2:\n",
    "        result = n\n",
    "    else:\n",
    "        result = fib(n-1) + fib(n-2)\n",
    "    cache[n] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda679b",
   "metadata": {},
   "source": [
    "We can also use the `functools` module, which comes with Python and provides the function decorator `@lru_cache`. LRU stands for Least Recently Used - when the cache hits its size limit it removes the least recently used data to make room for the new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eac273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def fib_lru(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
