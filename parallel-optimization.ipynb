{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1939a4ee",
   "metadata": {},
   "source": [
    "# Parallel Optimization in Python\n",
    "\n",
    "Parallelization allows you to speed up your code by running multiple operations at the same time, taking advantage of multiple CPU cores. In Python, this is especially useful for CPU-bound tasks, where the main bottleneck is computation rather than I/O.\n",
    "\n",
    "We will cover two popular approaches:\n",
    "1. `concurrent.futures` (standard library)\n",
    "2. `joblib` (external library, often used in scientific computing)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Parallelize?\n",
    "\n",
    "Python's default execution model (the Global Interpreter Lock, or GIL) means that only one thread executes Python bytecode at a time. However, you can use **process-based parallelism** to bypass the GIL for CPU-bound tasks. Essentially, multiple Python interpreters are created, each with their own GIL, allowing you to get around the lock. \n",
    "\n",
    "Of course, the first step is to profile your code and consider how parallelisable your algorithm is. Are there dependencies between steps in your code (i.e. step 2 depends on the result of step 1, etc.)? How much of your code can be parallelised, and how long do you think it would take?\n",
    "\n",
    "Below, we provide code for two different tasks. The first is the familiar Monte Carlo algorithm for estimating $\\pi$. The second is Euler's method for solving an ODE ([if you're not familiar](https://en.wikipedia.org/wiki/Euler_method))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81351803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e36a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def monte_carlo_pi(num_samples):\n",
    "    inside_circle = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if x**2 + y**2 <= 1:\n",
    "            inside_circle += 1\n",
    "    return (inside_circle / num_samples) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_ode(f, y0, t0, t1, dt):\n",
    "    #' Solves the ODE dy/dt = f(t, y) using Euler's method.\n",
    "    #' f is the function, y0 the initial value, t0 the start time,\n",
    "    #' t1 the end time, and dt the time step.\n",
    "    n_steps = int((t1 - t0) / dt)\n",
    "    t = np.linspace(t0, t1, n_steps + 1)\n",
    "    y = np.zeros(n_steps + 1)\n",
    "    y[0] = y0\n",
    "    for i in range(n_steps):\n",
    "        y[i + 1] = y[i] + f(t[i], y[i]) * dt\n",
    "    return t, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722ebfd",
   "metadata": {},
   "source": [
    "\n",
    "**Q1.1 How easy is it to parallelise `monte_carlo_pi()`? How easy do you think it is to parallelise `euler_ode()`?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Consider the dependencies between steps in the two functions.\n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "`monte_carlo_pi` generates lots of *independent* random numbers then tests whether they're within the boundary. `euler_ode` calculates one derivative, then updates *based on the derivative*, then calculates the next.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99828c",
   "metadata": {},
   "source": [
    "### How many CPU cores do you have?\n",
    "If you're not sure, you can use `import os; print(os.cpu_count())` in a REPL or Python script to find out.\n",
    "\n",
    "**Important!** Almost everyone has laptops with >1 CPU nowadays. If you only have 1 CPU, this section won't work. Please talk to me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827229fa",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelization with `concurrent.futures`\n",
    "\n",
    "The `concurrent.futures` module provides a high-level interface for asynchronously execution using threads *or* processes. The nice part is that it handles process/thread cleanup for you, and of course it is in the standard library so you don't need to download anything (sometimes a concern). If you need lower-level control or advanced features, check out the `multiprocessing` module in the Python standard library.\n",
    "\n",
    "To use `concurrent.futures` (after importing), we use a `ProcessPoolExecutor`, which will handle all the word behind the scenes. There are a few things to note:\n",
    "1. By default this uses all the CPUs reported by `os.cpu_count()`, i.e. all of the available CPUs. Set the `max_workers` kwarg to use less.\n",
    "2. Only objects that can be *pickled* (i.e. serialised) can be executed and returned from processes. Top-level functions are usually pickleable, but lambdas, local functions, open file handles etc. are not. If you're unsure, you can try to `pickle.dumps()` the object you're unsure about - if it doesn't raise an exception it should be fine.\n",
    "3. `ProcessPoolExector` does not work in interactive interpreters (like notebooks), because the `__main__` module needs to be importable to the workers. In notebooks, `__main__` is not a real, importable module like it is in scripts. \n",
    "\n",
    "As such, the rest of this section will describe code, but the main work should be done in `.py` scripts and called from the CLI.\n",
    "\n",
    "\n",
    "### Example: Parallel Square Calculation\n",
    "\n",
    "```python\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def slow_square(x):\n",
    "    time.sleep(1)\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t1 = time.perf_counter()\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results = list(executor.map(slow_square, range(5)))\n",
    "    t2 = time.perf_counter()\n",
    "    print(f\"Parallel execution took {t2 - t1:.4f} seconds\")\n",
    "    print(\"Results:\", results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d5af46",
   "metadata": {},
   "source": [
    "\n",
    "**Q1.2 How fast does this code run serially, and in parallel?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Copy the code to a `.py` file and run it with no concurrency, and with concurrency. Time it.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f9671",
   "metadata": {},
   "source": [
    "\n",
    "### Using `ThreadPoolExecutor` vs `ProcessPoolExecutor`\n",
    "\n",
    "- `ThreadPoolExecutor`: Best for I/O-bound tasks (e.g., network requests, file I/O).\n",
    "- `ProcessPoolExecutor`: Best for CPU-bound tasks (e.g., numerical computation).\n",
    "\n",
    "**Q1.3: Try swapping `ProcessPoolExecutor` for `ThreadPoolExecutor`. What happens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c4eee",
   "metadata": {},
   "source": [
    "### Submitting Tasks Individually\n",
    "\n",
    "You can also submit tasks one at a time and collect results as they finish:\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def slow_cube(x):\n",
    "    time.sleep(1)\n",
    "    return x ** 3\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(slow_cube, i) for i in range(5)]\n",
    "    for future in as_completed(futures):\n",
    "        print(future.result())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e1686",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We're going to parallelise the `monte_carlo_pi` function, which uses the Monte Carlo algorithm to estimate $\\pi$ by estimating the proportion of random numbers generated in the unit square which fall within the unit circle. \n",
    "\n",
    "You'll want to copy the previously provided code to a separate script, if you haven't already.\n",
    "\n",
    "\n",
    "**Q1.4 Modify the provide monte carlo code to be parallel. How long does it take when you use 2 cores, vs 1? How does the speed scale as the number of cores increases?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Copy the code to a script, and use the `concurrent.futures` library to run in parallel. \n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "You'll want to edit the code so it doesn't return the estimate of $\\pi$ directly, and use `ProcessPoolExecutor` as shown above. Collate the number of points within the unit circle, and calculate $\\pi$. <details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```python\n",
    "import random, os, time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def monte_carlo_pi(num_samples):\n",
    "    inside_circle = 0\n",
    "    for _ in range(num_samples):\n",
    "        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "        if x**2 + y**2 <= 1:\n",
    "            inside_circle += 1\n",
    "    return inside_circle\n",
    "\n",
    "def parallel_monte_carlo_pi(num_samples, num_processes):\n",
    "    samples_per_process = num_samples // num_processes\n",
    "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
    "        results = list(executor.map(monte_carlo_pi, [samples_per_process] * num_processes))\n",
    "    print(f\"{len(results)} processes completed.\")\n",
    "    return (sum(results) / num_samples) * 4\n",
    "```\n",
    "\n",
    "</details>\n",
    "</details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ad42a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Parallelization with `joblib`\n",
    "\n",
    "`joblib` is a third-party library that makes parallelization simple, with a much simpler interface.\n",
    "\n",
    "Returning to our example:\n",
    "\n",
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def slow_square(x):\n",
    "    time.sleep(1)\n",
    "    return x * x\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(slow_square)(i) for i in range(5))\n",
    "print(results)\n",
    "```\n",
    "\n",
    "- `delayed` is a `Joblib` provided function that wraps our function so it can be scheduled, rather than executing immediately.\n",
    "    - `for i in range(5)` creates 5 delayed tasks\n",
    "- `Parallel(n_jobs = 2)` creates a parallel executor, which in this case uses 2 worker processes.\n",
    "    - `n_jobs` specifies the number of processes to use (`-1` uses all available CPUs).\n",
    "\n",
    "**Q2.1:  I hope you agree that the `slow_square(x)` function should take ~1 second per call. How long do you think the example parallel function will take? Try it. Were you correct?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Copy the code to a `.py` file and run it with timing. \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9db239",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Exercise\n",
    "\n",
    "**Q2.2 Use `Joblib` to parallelise the `monte_carlo_pi` function. Which version do you prefer?**\n",
    "\n",
    "<details>\n",
    "<summary>I need a hint</summary>\n",
    "\n",
    "Copy the code to a script, and use the `Joblib` library to run in parallel. \n",
    "<details>\n",
    "<summary>I need a bigger hint</summary>\n",
    "\n",
    "You'll want to edit the code so it doesn't return the estimate of $\\pi$ directly, and use `delayed` and `Parallel` as shown above. Collate the number of points within the unit circle, and calculate $\\pi$. <details>\n",
    "<summary>Give me the code</summary>\n",
    "\n",
    "```python\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def parallel_monte_carlo_pi(num_samples, num_processes):\n",
    "    samples_per_process = num_samples // num_processes\n",
    "    results = Parallel(n_jobs=num_processes)(delayed(monte_carlo_pi)(samples_per_process) for _ in range(num_processes))\n",
    "    return (sum(results) / num_samples) * 4\n",
    "```\n",
    "\n",
    "</details>\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1380c00",
   "metadata": {},
   "source": [
    "# Final Exercise\n",
    "\n",
    "We will end here, as we have covered a fair few different techniques for speeding up code. Your final exercise, assuming there is enough time, is to *either*:\n",
    "\n",
    "1. Work on your own code. Profile it, and see if and where you can apply the things you have learned in this workshop, and any speedup that results.\n",
    "2. Work on the `unique_paths.py` code. This code generates the number of unique paths from the top-left to the bottom-right, in an `m * n` grid, when you can only move right or down. Time and profile this, and see if you how fast you can make it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
